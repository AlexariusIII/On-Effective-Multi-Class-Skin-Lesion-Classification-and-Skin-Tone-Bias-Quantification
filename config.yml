# Training
learning_rate: 0.0001
weight_decay: 0.01
batch_size: 128
num_epochs: 50

# Optimizer
optimizer: "AdamW"
scheduler: "ReduceLROnPlateau"
scheduler_patience: 2
scheduler_factor: 0.5

# Data
image_size: 384
num_workers: 4

# Augmentation
mix_up: False
# Gating networks
embed_dim : 128